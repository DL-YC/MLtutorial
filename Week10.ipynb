{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11\n",
    "\n",
    "## Learning with Large Datasets\n",
    "\n",
    "\"It's not who has the best algorithm that wins.  It's who has the most data.\"\n",
    "\n",
    "But large datasets come with their own problems.\n",
    "\n",
    "m = 100,000,000\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m}  \\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})) -y^{(i)})x_j^{(i)}  $$\n",
    "\n",
    "With m = 100,000,000, that's a lot of computation above.\n",
    "\n",
    "Why not use 1,000 examples?\n",
    "\n",
    "To check, plot learning curve (error vs. m), for J_{CV} and J_{train}\n",
    "\n",
    "In large scale machine learning, we need special tools for working with massive datasets.\n",
    "\n",
    "## Stochastic Gradient descent\n",
    "\n",
    "Modification to normal gradient descent, called stochastic gradient descent.\n",
    "\n",
    "### Batch gradient descent\n",
    "\n",
    "Linear regression with batch gradient descent:\n",
    "\n",
    "$$h_{\\theta}(x) = \\sum_{j=0}^{n}\\theta_jx_j$$\n",
    "\n",
    "$$J_{train}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2) $$\n",
    "\n",
    "Repeat {\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m}  \\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})) -y^{(i)})x_j^{(i)}  $$\n",
    "\n",
    "for every $j=0,...,n$\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "if M = 300 million, then this is very expensive.  We're looking at _batch_ gradient descent.\n",
    "\n",
    "You would need to read through each 300,000,000 records just to accumulate some value, and then only be able to take one step.\n",
    "\n",
    "\n",
    "### Stochastic gradient descent\n",
    "\n",
    "Linear regression with stochastic gradient descent:\n",
    "\n",
    "\n",
    "$$cost(\\theta,(x^{(i)}, y^{(i)})) = \\frac{1}{2}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "$$ J_{train}(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}cost(\\theta,(x^{(i)},y^{(i)})) $$\n",
    "\n",
    "1. Randomly shuffle dataset\n",
    "\n",
    "2. Repeat {\n",
    "\n",
    "for $i = 1,...,m$ {\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha(h_{\\theta}(x^{(i)} - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "(for $j = 0,...,n)\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "}\n",
    "\n",
    "Always best to shuffle the dataset first.\n",
    "\n",
    "Stochastic gradient descent will meander around minima, but won't always decrease.\n",
    "\n",
    "It's possible that you only need to pass through dataset once, or might need to pass through dataset ~10 times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "Can sometimes work even faster than stochastic gradient descent.\n",
    "\n",
    "- Batch gradient descent: use all _m_ examples in each iteration\n",
    "- Stochastic gradient descent: use 1 example in each iteration\n",
    "- Mini-batch gradient descent: use _b_ examples in each iteration\n",
    "\n",
    "## Stochastic Gradient Descent Convergence\n",
    "\n",
    "Batch gradient descent:\n",
    "\n",
    "- Plot $J_{train}$ as a function of the number of iterations of gradient descent\n",
    "\n",
    "Stochastic gradient descent:\n",
    "\n",
    "- During learning, compute cost before updating $\\theta$\n",
    "- Every 1000 iterations (say), plot cost averaged over last 1000 examples processed by algorithm\n",
    "\n",
    "Can slowly decrease $\\alpha$ over time to help converge\n",
    "\n",
    "## Online learning\n",
    "\n",
    "New setting - the online learning setting where we have data coming in real time.\n",
    "\n",
    "If you have a continuous stream of data, you can use that to optimize your web services, etc.\n",
    "\n",
    "## Map-reduce and data parallelism\n",
    "\n",
    "Some machine learning problems are too big to run on one machine.\n",
    "\n",
    "Map-reduce is maybe an equally important idea to stochastic gradient descent.\n",
    "\n",
    "Map... reduce...\n",
    "\n",
    "### Map-reduce and summation over the training set\n",
    "\n",
    "Many learning algorihtms can be expressed as computing sums of functions over the training set.\n",
    "\n",
    "E.g. for advanced optimization, with logistic regression, can express in summation form.\n",
    "\n",
    "### Multi-core machines\n",
    "\n",
    "Can also parallelize with multiple cores on one machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
